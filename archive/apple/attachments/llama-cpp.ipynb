{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 201 tensors from /Users/ammarh/j595/EmbodiedReasoning/Attention/FinetunedModels/output/checkpoint-250-synth-sentences-casper-generic/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = checkpoint-250\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type  f16:  156 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_layer          = 22\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5632\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 1.10 B\n",
      "llm_load_print_meta: model size       = 2.05 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = checkpoint-250\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/23 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2098.35 MiB\n",
      "..........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    11.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    66.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 710\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.model': 'llama', 'llama.vocab_size': '32000', 'llama.attention.head_count_kv': '4', 'llama.context_length': '2048', 'llama.attention.head_count': '32', 'general.file_type': '1', 'llama.feed_forward_length': '5632', 'llama.rope.dimension_count': '64', 'llama.rope.freq_base': '10000.000000', 'llama.embedding_length': '2048', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'checkpoint-250', 'llama.block_count': '22'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "#llm = Llama(model_path='/Users/ammarh/j595/llama.cpp/models/13B_vicuna_new/ggml-vic13b-q4_0.bin')\n",
    "# llm = Llama(model_path='/Users/ammarh/j595/llama.cpp/models/llama-2/llama-2-13b-chat.ggmlv3.q4_0.bin')\n",
    "\n",
    "# llm = Llama(model_path='/Users/ammarh/j595/llama.cpp/models/llama-2/llama-2-13b-chat.ggmlv3.q8_0.bin')\n",
    "#llm = Llama(model_path='/Users/ammarh/j595/Video-LLaMA-Series/finetune-vicuna13b-v2.pth')\n",
    "\n",
    "llamacpp_model = Llama(\"/Users/ammarh/j595/EmbodiedReasoning/Attention/FinetunedModels/output/checkpoint-250-synth-sentences-casper-generic/ggml-model-f16.gguf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-671fddc8-abd9-43e7-ae8d-e4fa57f20fba',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1692292296,\n",
       " 'model': '/Users/ammarh/j595/llama.cpp/models/llama-2/llama-2-13b-chat.ggmlv3.q8_0.bin',\n",
       " 'choices': [{'text': ' and takes a photo of George.\\nAlice: \"oh my god!\"\\nMary: \"hahaha that\\'s so great.\" \\n\\nHow would the Robot API function calls look like given this script? \\nI know it looks something like this but I\\'m not sure about the order, please help!\\n* say(\"ok!\")\\n* move_to(x,y,z) to pan around and open camera\\n* look_at(',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 412,\n",
       "  'completion_tokens': 100,\n",
       "  'total_tokens': 512}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:        load time = 40270.88 ms\n",
      "llama_print_timings:      sample time =    75.53 ms /   100 runs   (    0.76 ms per token,  1323.92 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40270.74 ms /   412 tokens (   97.74 ms per token,    10.23 tokens per second)\n",
      "llama_print_timings:        eval time = 2583604.17 ms /    99 runs   (26097.01 ms per token,     0.04 tokens per second)\n",
      "llama_print_timings:       total time = 2624336.21 ms\n"
     ]
    }
   ],
   "source": [
    "out = llm.create_completion(\"\"\"Here is an interaction that Alice, Mary and George have with a robot. The robot can be only be programmed by a Robot API that has the \n",
    "          following function calls -\n",
    "        ### def say(str)\n",
    "        ### def move_to(x,y,z)\n",
    "        ### def look_at(person)\n",
    "        ### make_sound(sound_type)\n",
    "Using just these API function calls for the robot, tell me in what order these function calls are made to the robot given the following sceript. \n",
    "Alice, Mary and George are humans. The Robot is a robot. Your output should only be using the Robot API function calls defined above.\n",
    "Alice: \"lets take a group photo\" (Alice in the center in front of Robot)\n",
    "Robot: \"ok!\" \n",
    "Robot pans around eyes open really quickly, and then opens the camera once back to facing Alice \n",
    "Robot: \"Can everyone get closer?\"\n",
    "Alice: \"Wait, actually guys, lets go here! next to the posters\" (Alice points and walks to a nicer spot)\n",
    "Robot was following Alice because she was the person they were tracking on launch\n",
    "Robot: \"Mary, look at the camera\" (Mary was drawing on her iPad)\n",
    "Robot: \"Everyone looks good, get ready, 3,.2.,..1\"\n",
    "Robot makes a shutter sound \n",
    "Robot: \"And lets zoom out for full body shots\" \n",
    "Robot makes a shutter sound with zoom out\n",
    "Alice: \"lets have a look at the photos.\"\n",
    "Mary: \"great, I love them.\"\n",
    "George: \"Oh wait! can I just take a selfie now?\" (George walks away from group, strikes a pose)\n",
    "Robot switches from \"group\" mode to solo mode, zooms back in\"\"\",\n",
    "          max_tokens=-1,)\n",
    "\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and takes a photo of George.\\nAlice: \"Wait, George has walked away!\"\\nRobot pans over to where George is standing for his selfie \\nGeorge: \"yeah yeah yeah! its done.\"\\nWhat order are the Robot API function calls made? \\nHere\\'s a hint - you only need to use two of the function calls.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VideoLlamaForPreTraining' from 'transformers' (/Users/ammarh/anaconda3/envs/afm3/lib/python3.9/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m VideoLlamaForPreTraining\n\u001b[1;32m      3\u001b[0m \u001b[39m#model = VideoLlamaForPreTraining.from_pretrained(\"damo/Video-LLaMA\")\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'VideoLlamaForPreTraining' from 'transformers' (/Users/ammarh/anaconda3/envs/afm3/lib/python3.9/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import VideoLlamaForPreTraining\n",
    "\n",
    "#model = VideoLlamaForPreTraining.from_pretrained(\"damo/Video-LLaMA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "cpt = torch.load(\"/Users/ammarh/Downloads/finetune-vicuna13b-v2.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_query_tokens\n",
      "Qformer.bert.embeddings.position_ids\n",
      "llama_model.model.layers.0.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.1.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.2.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.3.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.4.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.5.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.6.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.7.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.8.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.9.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.10.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.11.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.12.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.13.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.14.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.15.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.16.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.17.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.18.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.19.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.20.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.21.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.22.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.23.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.24.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.25.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.26.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.27.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.28.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.29.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.30.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.31.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.32.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.33.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.34.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.35.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.36.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.37.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.38.self_attn.rotary_emb.inv_freq\n",
      "llama_model.model.layers.39.self_attn.rotary_emb.inv_freq\n",
      "llama_proj.weight\n",
      "llama_proj.bias\n",
      "video_frame_position_embedding.weight\n",
      "video_Qformer.bert.embeddings.position_ids\n",
      "video_Qformer.bert.embeddings.LayerNorm.weight\n",
      "video_Qformer.bert.embeddings.LayerNorm.bias\n",
      "video_Qformer.bert.encoder.layer.0.attention.self.query.weight\n",
      "video_Qformer.bert.encoder.layer.0.attention.self.query.bias\n",
      "video_Qformer.bert.encoder.layer.0.attention.self.key.weight\n",
      "video_Qformer.bert.encoder.layer.0.attention.self.key.bias\n",
      "video_Qformer.bert.encoder.layer.0.attention.self.value.weight\n",
      "video_Qformer.bert.encoder.layer.0.attention.self.value.bias\n",
      "video_Qformer.bert.encoder.layer.0.attention.output.dense.weight\n",
      "video_Qformer.bert.encoder.layer.0.attention.output.dense.bias\n",
      "video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.self.query.weight\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.self.query.bias\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.self.key.weight\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.self.key.bias\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.self.value.weight\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.self.value.bias\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
      "video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
      "video_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight\n",
      "video_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias\n",
      "video_Qformer.bert.encoder.layer.0.output_query.dense.weight\n",
      "video_Qformer.bert.encoder.layer.0.output_query.dense.bias\n",
      "video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight\n",
      "video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias\n",
      "video_Qformer.bert.encoder.layer.1.attention.self.query.weight\n",
      "video_Qformer.bert.encoder.layer.1.attention.self.query.bias\n",
      "video_Qformer.bert.encoder.layer.1.attention.self.key.weight\n",
      "video_Qformer.bert.encoder.layer.1.attention.self.key.bias\n",
      "video_Qformer.bert.encoder.layer.1.attention.self.value.weight\n",
      "video_Qformer.bert.encoder.layer.1.attention.self.value.bias\n",
      "video_Qformer.bert.encoder.layer.1.attention.output.dense.weight\n",
      "video_Qformer.bert.encoder.layer.1.attention.output.dense.bias\n",
      "video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.self.query.weight\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.self.query.bias\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.self.key.weight\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.self.key.bias\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.self.value.weight\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.self.value.bias\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight\n",
      "video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias\n",
      "video_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight\n",
      "video_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias\n",
      "video_Qformer.bert.encoder.layer.1.output_query.dense.weight\n",
      "video_Qformer.bert.encoder.layer.1.output_query.dense.bias\n",
      "video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight\n",
      "video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "for k in cpt['model'].keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[0;32m----> 3\u001b[0m summary(cpt[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m], input_size\u001b[39m=\u001b[39;49m(\u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/afm3/lib/python3.9/site-packages/torchsummary/torchsummary.py:68\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m hooks \u001b[39m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m \u001b[39m# register hook\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m model\u001b[39m.\u001b[39;49mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m model(\u001b[39m*\u001b[39mx)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(cpt['model'], input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the FastAPI server\n",
    "url = \"http://localhost:8000/completion\"\n",
    "\n",
    "# Request payload\n",
    "payload = {\n",
    "    \"system_prompt\": \"how many planets are there?\",\n",
    "    \"max_tokens\": 50,\n",
    "}\n",
    "\n",
    "# Send POST request to the server\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "print(response.json()[\"completion\"].keys())\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    # Request successful\n",
    "    completion = response.json()[\"completion\"]\n",
    "    print(\"Generated Completion:\")\n",
    "    print(completion)\n",
    "else:\n",
    "    # Request failed\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "    print(\"Error message:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import MistralForCausalLM\n",
    "\n",
    "GPT2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tok = LlamaTokenizer(vocab_file=\"/Users/ammarh/Desktop/HF-models/TinyMistral-248M/tokenizer.model\")\n",
    "tok.tokenize(\"hello world\")\n",
    "\n",
    "model = MistralForCausalLM.from_pretrained(\"/Users/ammarh/Desktop/HF-models/TinyMistral-248M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a huggingface HF model on device from cloned repo\n",
    "import torch\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Specify the input prompt\n",
    "prompt = \"Where is Paris?\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tok.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the model's response\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=90,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.9,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode the generated response\n",
    "response = tok.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "ModelUrl = \"http://localhost:1234/completion\"\n",
    "\n",
    "with open('system-prompt-eval.txt', 'r') as file:\n",
    "    system_prompt = file.read()\n",
    "\n",
    "data = {\n",
    "    \"prompt\":system_prompt,\n",
    "    # \"grammar\": grammar,\n",
    "    \"n_predict\": -1,\n",
    "    # \"cache_prompt\": True,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.5,\n",
    "    \"n_predict\": 100\n",
    "}\n",
    "\n",
    "# print(f\"Sending this to LLM - {data['prompt']}\") \n",
    "\n",
    "response = requests.post(ModelUrl, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(data))\n",
    "response_data = json.loads(response.text)\n",
    "response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "mistral_llm = Llama(model_path='/Users/ammarh/j595/llama.cpp/models/VeryTinyLMs/TinyMistral-248M-f16.gguf', n_ctx=32768)\n",
    "\n",
    "# llama_llm = Llama(model_path='/Users/ammarh/j595/llama.cpp/models/VeryTinyLMs/TinyLlama-1.1B-intermediate-step-1431k-3T-f16.gguf', n_ctx=32768)\n",
    "\n",
    "# llama_llm.create_completion(system_prompt, max_tokens=20)\n",
    "\n",
    "mistral_llm.create_completion(system_prompt, max_tokens=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
