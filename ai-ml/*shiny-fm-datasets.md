---
created: 2022-08-07-Sunday 16:07
modified: 2023-09-19-Tuesday 15:5631
---

# Datasets
- [MIMIC-IT](https://github.com/Luodian/Otter/blob/main/mimic-it/README.md)
	- Offers a diverse and extensive dataset of 2.8M multimodal instruction-response pairs, designed to enhance the performance of Vision-Language Models (VLMs) in real-life scenarios, enabling VLMs to excel in perception, reasoning, and planning while also catering to a multilingual audience.
	- MIMIC-IT enables the application of egocentric visual assistant model that can serve that can answer your questions like¬†**Hey, Do you think I left my keys on the table?**.
- Open Assistant Conversations (OASST1)
- SA-1B data from [Segment Anything](https://github.com/facebookresearch/segment-anything)¬†
- Dolma is a 3 trillion-token dataset: [AI2 releases the largest open source text dataset for LLM pretraining](https://alphasignal.us18.list-manage.com/track/click?u=ff56eecaa32b24c3f057760a9&id=aacda97072&e=3f3a663a11)

[[2023-09-26-Tuesday]]
- **MosIT** [[*shiny-fm-vlm#NExT-GPT]]
	- To facilitate the development of any-to-any MM-LLM, we propose a novel Modality-switching Instruction Tuning (MosIT). MosIT not only supports complex cross-modal understanding and reasoning but also enables sophisticated multimodal content generation. In conjunction with MosIT, we manually and meticulously construct a high-quality dataset. The MosIT data encompasses a wide range of multimodal inputs and outputs, offering the necessary complexity and variability to facilitate the training of MM-LLMs that can handle diverse user interactions and deliver desired responses accurately. Specifically, we design some template dialogue examples between a ‚ÄòHuman‚Äô role and a ‚ÄòMachine‚Äô role, based on which we prompt the GPT-4 to generate more conversations under various scenarios with more than 100 topics or keywords. After human inspections and filtering of inappropriate instances, we obtain a total of 5K dialogues in high quality
- ShareGPT
- Open-Platypus , a small-scale dataset that consists of a curated sub-selection of public text datasets. The dataset is focused on improving LLMs‚Äô STEM and logic knowledge, and is made up of 11 open-source datasets. It is comprised mainly of human-designed questions, with only 10% of questions generated by an LLM. The main advantage of Open-Platypus is that, given its size and quality, it allows for very strong performance with short and cheap fine-tuning time and cost. Specifically, one can train their own 13B model on a single A100 GPU using 25k questions in 5 hours.
	- Core contribution of [[*shiny-fm-llm#Platypus]]
	- ![[shiny-fm-datasets-2023-09-26.png]]
- Dolphin üê¨ [https://erichartford.com/dolphin](https://erichartford.com/dolphin)
	- This dataset is an attempt to replicate the results of¬†[Microsoft's Orca](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/)
	- Our dataset consists of:
		- ~1 million of FLANv2 augmented with GPT-4 completions (flan1m-alpaca-uncensored.jsonl)
		- ~3.5 million of FLANv2 augmented with GPT-3.5 completions (flan5m-alpaca-uncensored.jsonl)
	- We followed the submix and system prompt distribution outlined in the Orca paper. With a few exceptions. We included all 75k of CoT in the FLAN-1m dataset rather than sampling that. Also, we found that many items were duplicated, so we removed duplicates, resulting in 3.5m instructs in the ChatGPT dataset.
	- compare to FLAN-1Million and FLAN-5Million for [[*shiny-fm-llm#Orca - 13B]]

- **Multilingual data**: [CohereForAI/aya\_collection ¬∑ Datasets at Hugging Face](https://huggingface.co/datasets/CohereForAI/aya_collection)
[[2024-03-15-Friday]]
- [Instruction Tuning with GPT-4](https://instruction-tuning-with-gpt-4.github.io/)

## OpenOrca
Collection of augmented¬†[FLAN Collection data](https://arxiv.org/abs/2301.13688). Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is tabularized in alignment with the distributions presented in the ORCA paper and currently represents a partial completion of the full intended dataset, with ongoing generation to expand its scope.

## Open X-Embodiment dataset
[[2023-10-03-Tuesday]]
Together with partners from 33 academic labs we have pooled data from 22 different robot types to create the Open X-Embodiment dataset. We also release RT-1-X, a robotics transformer (RT) model derived from¬†[RT-1](https://blog.research.google/2022/12/rt-1-robotics-transformer-for-real.html)¬†and trained on our dataset, that shows skills transfer across many robot embodiments.

## Self-Instruct
[[2023-11-18-Saturday]]
[GitHub - yizhongw/self-instruct: Aligning pretrained language models with instruction data generated by themselves.](https://github.com/yizhongw/self-instruct)
Methodology to generate synthetic data using an LM (GPT-3)

## FineWeb
[[2024-04-29-Monday-W18|2024-04-29-Monday]]: HuggingFace releases [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) a 15 trillion tokens of high quality web data: 
This seems to be the go-to high quality filtered dataset of choice. Has variants extracted from it like:
[HuggingFaceFW/fineweb-edu ¬∑ Datasets at Hugging Face](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)
**a high-quality subset of the 15 trillion token FineWeb dataset**, created by¬†**filtering FineWeb using a Llama 3 70B model to judge educational quality**. It enables¬†**better and faster LLM learning**.
[FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1?__readwiseLocation=)
## HelpSteer2
[Paper](https://arxiv.org/pdf/2406.08673)
Dataset for doing RLHF
CC-BY-4.0-licensed open-source helpfulness dataset, designed to train state-of-the-art reward models. We provide detailed information about our data collection process to aid similar efforts and demonstrate how reward models trained with HelpSteer2
can align large language models with human preferences.
Used in [[*shiny-fm-llm#Nemotron-4-340B-Base]] for reward modeling
# Dummy Datasets
![[TinyStories_train.txt]]![[TinyStories_valid.txt]]
![[code-junkyard/turi/shakespeare.txt]] Corresponding data prep and tokenizing script for shakespeare - [prepare.py](https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare/prepare.py)
