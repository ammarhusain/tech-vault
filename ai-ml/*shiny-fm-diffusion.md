---
created: 2023-09-12-Tuesday 11:09
modified: 2023-09-12-Tuesday 11:10
---
# Models
## DALL·E 2

([DALL·E 2](https://openai.com/dall-e-2/)) #sandbox

- A new AI system that can create realistic images and art from a description in natural language. It has learned the relationship between images and the text used to describe them. It uses a process called “diffusion,” which starts with a pattern of random dots and gradually alters that pattern towards an image when it recognizes specific aspects of that image.
[[2022-12-09-Friday]] - Super interesting book on how to prompt these models: [The DALL·E 2 prompt book](https://pitch.com/v/DALL-E-prompt-book-v1-tmd33y)

## Imagegen

[Imagen: Text-to-Image Diffusion Models](https://imagen.research.google/)
[Demo](https://media-gen.corp.google.com/)
A text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large [[transformer]] language models in understanding text and hinges on the strength of [[diffusion model]] in high-fidelity image generation.
![[Pasted image 20220825121445.png]]
![[Pasted image 20220902113633.png]]

- Different from [[*shiny-fm-datasets#DALL·E 2]] in that it uses a language only encoder for text rather than the [[*shiny-fm-datasets#CLIP Connecting Text Images]] joint text & image embedding that DALLE-2 uses
- Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model.
- Imagen uses a large frozen T5-XXL encoder to encode the input text into embeddings. A conditional diffusion model maps the text embedding into a 64×64 image. Imagen further utilizes text-conditional super-resolution [[diffusion model]] to upsample the image 64×64→256×256 and 256×256→1024×1024.
- [[2022-10-12-Wednesday]] : Also has a video version now [Imagen Video](https://imagen.research.google/video/)

## Glide: towards Photorealistic Image Generation and Editing with Text-guided [[diffusion model]]

No blog post surprisingly, [paper](https://arxiv.org/abs/2112.10741)
[GitHub - openai/glide-text2im: GLIDE: a diffusion-based text-conditional image synthesis model](https://github.com/openai/glide-text2im)
[Google Colab - text2im](https://colab.research.google.com/drive/1ZnZz8j5rjGGHrjBp2yosEKeZ0lGy5blQ?authuser=1) #sandbox
[Google Colab - inpaint](https://github.com/openai/glide-text2im/blob/main/notebooks/inpaint.ipynb)

## Parti

[Parti: Pathways Autoregressive Text-to-Image Model](https://parti.research.google/) [Demo](https://media-gen.corp.google.com/)

- Autoregressive text-to-image generation model that achieves high-fidelity photorealistic image generation and supports content-rich synthesis involving complex compositions and world knowledge. 
- Parti and [[*shiny-fm-datasets#ImageGen]] are complementary in exploring two different families of generative models – autoregressive and diffusion, respectively – opening exciting opportunities for combinations of these two powerful models.
- Parti treats text-to-image generation as a sequence-to-sequence modeling problem, analogous to [[courses/xcs224n - natural language processing/8-9 - Machine Translation, Seq2Seq and Attention|machine translation]] – this allows it to benefit from advances in large language models, especially capabilities that are unlocked by scaling data and model sizes. In this case, the target outputs are sequences of image tokens instead of text tokens in another language.

![[Pasted image 20220902115138.png]]

- Uses [[*shiny-fm-datasets#Vector-Quantized Image Modeling with Improved VQGAN]] as image tokenizer

## Phenaki

[[2022-10-12-Wednesday]] : [[diffusion model]]
Meta's [Make-A-Video](https://substack.com/redirect/34932082-8ed7-4520-b5dd-fc9b515448f7?r=f2u90) can take a still image, and then generate a video that continues the image. It can also generate variations of a video. Moreover, the new model Phenaki by Google Brain can take a series of prompts and generate a single continuous video that make up a story in which each prompt occurs sequentially. For instance, in the above example, Phenaki generates a video in which a teddy bear is first swimming in the ocean, then goes under water, continues swimming underwater, and then morphs into a panda bear underwater. On the [demo website](https://substack.com/redirect/be649577-6059-42f3-b417-b0612a56e0a7?r=f2u90), there are longer videos spanning over 2 minutes that tell a continuous story generated by a longer sequence of prompts.

## Diffrf

[[diffusion model]] - 3D [DiffRF: Rendering-guided 3D Radiance Field Diffusion](https://sirwyver.github.io/DiffRF/)

## Dreamfusion

[[diffusion model]] - 3D [DreamFusion: Text-to-3D using 2D Diffusion](https://dreamfusion3d.github.io/)

## Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models - NVIDIA

[Article](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)
Might be relevant to [[wearscape-ai]]

- Focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling.
- can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048.
- Our Video LDMs map videos into a compressed latent space and model sequences of latent variables corresponding to the video frames (see animation ). We initialize the models from image LDMs and insert temporal layers into the LDMs' denoising neural networks to temporally model encoded video frame sequences. The temporal layers are based on temporal attention as well as 3D convolutions. We also fine-tune the model's decoder for video generation (see figure)

![[shiny new models-2023-05-03.png|600]]

- *Top:* During temporal decoder fine-tuning, we process video sequences with a frozen per-frame encoder and enforce temporally coherent reconstructions across frames. We additionally employ a video-aware discriminator. 
- *Bottom:* in LDMs, a diffusion model is trained in latent space. It synthesizes latent features, which are then transformed through the decoder into images. Note that in practice we model entire videos and video fine-tune the latent diffusion model to generate temporally consistent frame sequences.

![[shiny new models-2023-05-03.mp4|500]]

## 3DFuse

Potentially useful for [[wearscape-ai]]
![[shiny new models-2023-05-03-1.png]]
Novel framework that incorporates 3D awareness into pretrained 2D diffusion models, enhancing the robustness and 3D consistency of score distillation-based methods. We realize this by first constructing a coarse 3D structure of a given text prompt and then utilizing projected, view-specific depth map as a condition for the diffusion model.

![[shiny new models-2023-05-03-2.png]]

## DALL-E 3
[[2023-09-26-Tuesday]] - [DALL·E 3](https://openai.com/dall-e-3)
DALL·E 3 is built natively on ChatGPT, which lets you use ChatGPT as a brainstorming partner and refiner of your prompts. Just ask ChatGPT what you want to see in anything from a simple sentence to a detailed paragraph.
When prompted with an idea, ChatGPT will automatically generate tailored, detailed prompts for DALL·E 3 that bring your idea to life. If you like a particular image, but it’s not quite right, you can ask ChatGPT to make tweaks with just a few words.

## Sora
[[2024-02-15-Thursday]] - [Sora](https://openai.com/sora)
Text to video model - creating a lot of buzz.