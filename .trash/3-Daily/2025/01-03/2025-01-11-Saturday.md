---
alias: ["Saturday, Jan 11, 2025"]
tags: 
---
# TOP-OF-MIND
- #aiml 
	- [What do the gods of generative AI have in store for 2025? - Economist](https://read.readwise.io/new/read/01jh6r78hdkwbrygarrcdxdcfp) 
	- [ðŸŒ#80: What's in 2025? From Elad Gil, FrancÌ§ois Chollet, Maxime Labonne, swyx and others](https://www.turingpost.com/p/fod80?__readwiseLocation=)
		- â€œIt's going to be the year of inference-time search, and we will see many efforts to create an open source reproduction of O1. Also ARC-AGI will be solved.â€
		- Flow engineering/"guided chain of thought" will be deployed much more in production than o1/reasoning type models.â€ 
		- â€œEdge LLMs will go from feasible to popular, thanks to progress with small language models and optimized inference.â€
	- [OpenAI o3 Breakthrough High Score on ARC-AGI-Pub](https://arcprize.org/blog/oai-o3-pub-breakthrough?__readwiseLocation=)
		- ARC-AGI-1 took 4 years to go from 0% with GPT-3 in 2020 to 5% in 2024 with GPT-4o. All intuition about AI capabilities will need to get updated for o3.
		- o3's improvement over the GPT series proves that architecture is everything. You couldn't throw more compute at GPT-4 and get these results. Simply scaling up the things we were doing from 2019 to 2023 â€“ take the same architecture, train a bigger version on more data â€“ is not enough. Further progress is about new ideas.
		- early data points suggest that the upcoming ARC-AGI-2 benchmark will still pose a significant challenge to o3, potentially reducing its score to under 30% even at high compute (while a smart human would still be able to score over 95% with no training). This demonstrates the continued possibility of creating challenging, unsaturated benchmarks without having to rely on expert domain knowledge. You'll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.
		- Effectively, o3 represents a form of deep learning-guided program search. The model does test-time search over a space of "programs" (in this case, natural language programs â€“ the space of CoTs that describe the steps to solve the task at hand), guided by a deep learning prior (the base LLM). The reason why solving a single ARC-AGI task can end up taking up tens of millions of tokens and cost thousands of dollars is because this search process has to explore an enormous number of paths through program space â€“ including backtracking.



# TASKS COMPLETED TODAY
%% TCT_TEMPLATED_START 2025-01-11 00:00 %%
%% TCT_TEMPLATED_END 2025-01-11 23:59 %%


# NOTES CREATED IN THE LAST WEEK
Removed for today because Created time is messed up for a lot of files after a refactor

# NOTES MODIFIED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.mtime AS Modified, file.ctime AS Created
WHERE file.mtime >= date(substring(this.file.name,0,10)) - dur(1 week)
AND file.mtime < date(substring(this.file.name,0,10))
AND file.ctime < date(substring(this.file.name,0,10)) - dur(1 week)
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```
---
