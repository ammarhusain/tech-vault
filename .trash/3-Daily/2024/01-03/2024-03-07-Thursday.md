---
aliases: [2024-03-07]
tags: 
---
**[DayOne](dayone://open?date=2024-03-07)**

# TOP-OF-MIND
- #j595/journal 
	- Trying to methodically debug llama.cpp interactive and chatml interface for [[attentionkit-fc]] to bump down response latency
	- Here is an example command: ./main -m models/dolphin-2.7-mixtral-8x7b/dolphin-2.7-mixtral-8x7b.Q5_K_M.gguf -p $ATTNPROMPT --chatml -c 0 --grammar-file grammars/attn_json.gbnf -r "}]}" -r "<|im_end|>"
	- Did a lot of step by step digging through to get chatml and interactive mode working - tried both between app & superserver. What really helped was to try out without the complexity of the swift tool or palazzo-llm-ws but just with barebones llama.cpp before starting to build up complexity. 
		- Always start with the simplest way first before building up complexity! - #j595/meditation 
	- Few interesting discoveries:
		- chatml adds a few extra new lines and a '>' character so those need to get accounted for when streaming back
		- palazzo-llm-ws has a whole bunch of extra logic and one was that it would only create a stdin stream when the interactive flag is passed not when chatml is passed. This caused the stream to be null and caused websocket to crash when the model responds
		- in interactive mode the longer context history is causing the llm to go off the rails a lot more and gets stuck in a bad result. For ex: it did not detect me once and said "I do not see anyone" and then it kept spewing out the same result as new invocations came to it. 
		- > [!tldr] interactive chat might not be the right approach for attention - probably need to make one shot prompting faster instead.
		- #tinker_old try keeping llm primed & ready before audio msg: and then send one interactive msg (that might still be 2.5ish secs though - first few invocations of interactive chat)
		- With chatml flag on llama.cpp getting round trip 2.5-3.8secs (depending on number of entities in view 0 or 1). Responses are pretty generic & garbage however.
		- Without chatml and manually injecting <|im_start|> and <|im_end|> tokens the responses are faster but equally crappy performance wise - it sticks to grammar but unable to really do much chain-of thought
		- Using a server and dolphin2.7 mixtral8x7B model getting around ~40 tokens per second
			- ![[2024-03-07-Thursday-2024-03-07.png]]
	- Tried using llama.cpp server endpoint instead which seems to work pretty well so far - got latency down to 2.6 seconds by just testing it with one shot equivalent prompt:
		- Here is the command to run it on the terminal to test: [[j595 cheatsheet#Llama.cpp]]
		- This is a promising approach - need to integrate it within the app next!
		- client side: ![[2024-03-07-Thursday-2024-03-07-1.png]]
		- server side: ![[2024-03-07-Thursday-2024-03-07-2.png]]
		- 2.7s to get response and ~40 tokens per sec  for generation - add threads, mlocks and certain other flags to see if you can bump it even more. The actual generation time by the model is around 1.6s so room to shave off a bit more.


# NOTES CREATED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.ctime As Created
WHERE file.ctime >= date(substring(this.file.name,0,10)) - dur(1 week) 
AND file.ctime < date(substring(this.file.name,0,10)) 
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```

# NOTES MODIFIED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.mtime AS Modified, file.ctime AS Created
WHERE file.mtime >= date(substring(this.file.name,0,10)) - dur(1 week)
AND file.mtime < date(substring(this.file.name,0,10))
AND file.ctime < date(substring(this.file.name,0,10)) - dur(1 week)
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```
---
