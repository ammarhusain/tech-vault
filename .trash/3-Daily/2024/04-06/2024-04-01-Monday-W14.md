---
aliases:
  - 2024-04-01-Monday
  - 2024-04-02-Tuesday
  - 2024-04-03-Wednesday
  - 2024-04-04-Thursday
  - 2024-04-05-Friday
  - 2024-04-06-Saturday
  - 2024-04-07-Sunday
tags: 
publish: false
---
**[DayOne](dayone://open?date=2024-04-01)**

# TOP-OF-MIND
- #j595/journal 
	- [[2024-04-01-Monday]] - working to setup a python evaluation script for attention.
		- Tried Mixtral-Instruct instead of the dolphin model - seems pretty good. Played around with prompt token structuring.
	- [[2024-04-02-Tuesday]] - Updated device & Xcode; 
		- tried to get PR out on the server based attention LLM with the system prompt token cleanup
		- messed up my git rebasing and got frustrated trying to pull main back into my changes. I should have squashed first to get everything into one commit before rebasing and resolving over & over again. Also need a better diff view where I can easily merge changes
	- [[2024-04-03-Wednesday]] - 
		- Got my PR ready to go - updated device & xcode to 197 of Palazzo but now I dont get Motif track in sigmon consumer though I see it when streaming on device.
		- Prepared for the literature review presentation tomorrow of the following 2 papers: [[*shiny-fm-llm#1-bit LLMs]]
	- [[2024-04-04-Thursday]]
		- Contentious meeting with Hans & Luke on attention and their vision of it
		- Presented [[*shiny-fm-llm#[BitNet Scaling 1-bit Transformers for Large Language Models](https //arxiv.org/pdf/2310.11453.pdf)]] at literature review
		- Left early to get smog test done for [[car|frankie]] 
		- Came home and watched 3blue1brown attention video
	- [[2024-04-05-Friday]]
		- [ ] Attention should output an entity + action where the action gets registered by the behavior. This would be an interesting prototype on the Attention LLM #j595/not-todo  
		- Ran swift format on files to submit another PR - disentangles formatting changes from my server endpoint change
		- Walking through the [[karpathy]] tutorial again to try understand in depth
		- Working towards getting a grasp of karpahy's llama.c library
			- Main difference between GPT-2 and Llama-2 architecture is the following:
				- Llama-2 uses alibi attention which pretty much means adding a position encoding to the attention tokens as well - these encodings are learnable and makes the system try to figure out tokens at what positions are meaningful
				- RoPe positional embedding which is not learned compared to the GPT positional embedding which is learned
				- Llama-2 uses SentencePiece tokenizer yielding a vocab of ~34k vs Byte Pair encoding for GPT with a vocab of ~50k
				- Llama-2 incorporates Parallel Residual Connections in the feed forward network which just seems like a multi headed feed forward layer to me. Split the linear layer + activation into N different chunks and then concatenate them before running them through a final projection layer.
				- Llama-2 uses root mean square layer norm (RMSLayerNorm) vs the regular layerNorm in GPT

# NOTES CREATED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.ctime As Created
WHERE file.ctime >= date(substring(this.file.name,0,10)) - dur(1 week) 
AND file.ctime < date(substring(this.file.name,0,10)) 
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```

# NOTES MODIFIED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.mtime AS Modified, file.ctime AS Created
WHERE file.mtime >= date(substring(this.file.name,0,10)) - dur(1 week)
AND file.mtime < date(substring(this.file.name,0,10))
AND file.ctime < date(substring(this.file.name,0,10)) - dur(1 week)
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```
---
