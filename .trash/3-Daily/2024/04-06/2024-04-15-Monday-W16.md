---
aliases:
  - 2024-04-15-Monday
  - 2024-04-16-Tuesday
  - 2024-04-17-Wednesday
  - 2024-04-18-Thursday
  - 2024-04-19-Friday
  - 2024-04-20-Saturday
  - 2024-04-21-Sunday
tags: 
publish: false
---
**[DayOne](dayone://open?date=2024-04-15)**

# TOP-OF-MIND
- [[2024-04-15-Monday-W16|2024-04-15-Monday]]
	- Called off on pursuing Comet
	- Got dynamic grammar generation working with *python*
- [[2024-04-15-Monday-W16|2024-04-16-Tuesday]]
	- Attended meeting from HI on [FC3 design considerations](https://quip-apple.com/9lzuA75KbQwM)
	- Anton brought up this idea of [Working Memory](https://quip-apple.com/10i7Aiffw0yB) which would be a great interface between different reasoning engines. Like Luke's global dialog model could tag its output and store it in Working memory - attention could then fetch & sequence it together through function calling.
	- Use a dynamic grammar for function calling as well where you could condition it on different acceptable function calls.
	- [x] #j595/todo Read the following hyperparameter tuning docs: [completed:: [[2024-04-17-Wednesday]]]
		- [x] [Top-k & Top-p](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p)  [completion:: 2024-04-17]
		- [x] [Token selection strategies: Top-K, Top-P, and Temperature](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/)  [completion:: 2024-04-17]
		- [x] [Understanding Key AI Language Model Parameters: top\_p, Temperature, num\_beams, and do\_sample | by Omar Santos | Medium](https://becomingahacker.org/understanding-key-ai-language-model-parameters-top-p-temperature-num-beams-and-do-sample-9874bf3c89ae)  [completion:: 2024-04-17]
		- [x] [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)  [completion:: 2024-04-17]
- [[2024-04-15-Monday-W16|2024-04-17-Wednesday]]
	- [x] #j595/todo Read & summarize MM1 [2403.09611.pdf](https://arxiv.org/pdf/2403.09611.pdf) [completed:: [[2024-04-17-Wednesday]]]
		- Is this something we can use for Attention in FC3 as an investigation?
	- Created notes [[token sampling]] by reading through a bunch of articles
	- Met Mihai Todor on Lunchclub - 
		- interesting conversation with a distributed systems engineer. He gave me an overview of what SnowFlake is and what it does. Its basically an advanced querying engine on large datasets where your data can be in S3 buckets anywhere and if you want to run analytics you can specify a T-shirt sized warehousing job (so not too technical) and then it finds a compute cluster in a nearby geographic region to fetch your data and run your queries. Additionally you can create dashboards etc on top of your data so very appealing to enterprise customers. Its not really a data selling / sharing platform though you could share your data product that is built on top of it (like an API or dashboard).
	- Productive chat with Xin learning about his function calling model for FC3:
		- He is trying to building a universal model for both dialog generation and function calling (like a ChatGPT). His function calling API however would sit on a higher level than the Attention model though. 
		- One could imagine in the future the AttentionLLM does not even listen to the audio directly but gets a summary through the dialog model in Working Memory. 
		- The Attention model operates on a more "action level. Problems are similar though and the attentionLLM is smaller and potentially running on device. While they are more focused in the language arena attentionLLM is more focused with onboard perception.
		- Its mostly him and Blanca working on it in FC3 and focused on data collection (& synthetic data gen). It would be great to sync with them periodically. Sri & Reza might be working on Memory.
		- We should probably phrase it as the Mini/modest mouse vs Mighty Mouse discussion where what Xin builds is the Mighty mouse server hosted while what we do is the Modest Mouse run on device.
	- Came across Griffin - supposedly a new architecture to Transformers that can potentially outperform it. There is also Mamba. So much going on in the [[ai-ml|aiml]] and [[*shiny-fm-llm]] space.
- [[2024-04-18-Thursday]]
	- Saw the J490 demo
	- Couldnt get in the flow. Played around with [guidance-ai/guidance: A guidance language for controlling large language models.](https://github.com/guidance-ai/guidance?tab=readme-ov-file#stateful-guidance-functions)
- [[2024-04-19-Friday]]
	- Read this [Robomem paper](https://arxiv.org/pdf/2003.10553.pdf) - Giving Long Term Memory to Robots : thought it would be interesting but its pretty outdated now 
	- Read a few other articles:
		- [Trusting Your Evidence: Hallucinate Less with Context-aware Decoding](https://arxiv.org/pdf/2305.14739.pdf)
		- [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)
		- [Towards 1-bit Machine Learning Models](https://mobiusml.github.io/1bit_blog/)
			- The experimental 2 and 1-bit quantized Llama2-7B models with a low-rank adapter, quantized using the proposed HQQ+ approach, showcase the potential of extreme low-bit quantization in machine learning models. Despite the challenges posed by such extreme settings, the fine-tuned models demonstrate significant improvements in output quality.
			- Methods like BitNet train the full network from scratch. Instead, we follow the direction of training low-rank adapters ([LoRA](https://arxiv.org/abs/2106.09685)/[QLoRA](https://arxiv.org/abs/2305.14314)), which is currently the most popular way for fine-tuning large models.


# NOTES CREATED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.ctime As Created
WHERE file.ctime >= date(substring(this.file.name,0,10)) - dur(1 week) 
AND file.ctime < date(substring(this.file.name,0,10)) 
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```

# NOTES MODIFIED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.mtime AS Modified, file.ctime AS Created
WHERE file.mtime >= date(substring(this.file.name,0,10)) - dur(1 week)
AND file.mtime < date(substring(this.file.name,0,10))
AND file.ctime < date(substring(this.file.name,0,10)) - dur(1 week)
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```
---
