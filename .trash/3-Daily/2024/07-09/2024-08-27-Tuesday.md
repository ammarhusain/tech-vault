---
alias: ["Tuesday, Aug 27, 2024"]
tags: 
---


# TOP-OF-MIND
- #journal/daily #travel/london 
	- Woke up at like 10.30am this morning - was super exhausted after the long flight + jetlag. Went to bed at midnight and other than being up for a bit around 2.30am passed out otherwise.
	- Setup running `llm` locally on the command line CLI - [Setup - LLM](https://llm.datasette.io/en/stable/setup.html#)
		- Used Karpathy's code to auto generate [git commit message based on diff](https://x.com/karpathy/status/1827810695658029262)
-  #crawlspace 
	- Applebot repo [link](https://github.pie.apple.com/applebot)
		- 13T discovered URLs and 400B documents in corpus - refreshed periodically
		- Document and WebDoc are the two main protobuf formats used in the Annex UI (applebot/protobufs)
		- Extracts Semantic Entities like: Places, Products, Videos, Conversations etc. Entities like Questions & Answers could be very relevant to extract
		- Extractors are a set of rules that transform web documents into a custom JSON
			- Refinery tab in Annex UI
			- semantic_web.structured_refinery in WebDoc
		- Extractors are like mini pipelines
			1. Source - pick html elements with a CSS selector `.review-container`
			2. Select/Transform - Map each elements to contents, attribute values etc
			3. Refine - Apply string operations (split), conversions (string to int) etc
			4. JSON output 
		- Once an extractor is defined you can take them and bundle a few into Datasets
			- This would be very useful functionality to have once we have a few publishers signed on
			- It provides versioning + other capabilities on the dataset
	- Productive chat with John Khalil pitching him on the vision of Crawlspace - he is certainly very interested to be involved both on the legal front as well as an ongoing advisor on startup strategy and business development.
	- Jakub in-person 1:1
		- [architecture diagram](https://drive.google.com/file/d/1KCiWCaap9kyiYu-bGC4Ww0kBW8GMmCMt/view?usp=sharing) of all the lambdas - [[crawlspace_architecture.drawio]]
		- `prod_page_renderer` is the lambda to run a crawl with the Chromium renderer
	- 
# TASKS COMPLETED TODAY
%% TCT_TEMPLATED_START 2024-08-27 00:00 %%
%% TCT_TEMPLATED_END 2024-08-27 23:59 %%


# NOTES CREATED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.ctime As Created
WHERE file.ctime >= date(substring(this.file.name,0,10)) - dur(1 week) 
AND file.ctime < date(substring(this.file.name,0,10)) 
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```

# NOTES MODIFIED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.mtime AS Modified, file.ctime AS Created
WHERE file.mtime >= date(substring(this.file.name,0,10)) - dur(1 week)
AND file.mtime < date(substring(this.file.name,0,10))
AND file.ctime < date(substring(this.file.name,0,10)) - dur(1 week)
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```
---
