---
alias: ["Friday, Oct 25, 2024"]
tags: 
---
# TOP-OF-MIND
- #journal/daily 
	- Went to DMV again - almost an hour late from my appointment time and was able to get everything done for the ReadID. Took and hour and a half or so.
	- Wrapped up evaluating the trained GPT2 models from [[karpathy#[Let's reproduce GPT-2 (124M) - YouTube](https //www.youtube.com/watch?v=l8pRSuU81PU)]] and added results
		- interesting discovery regarding the register_buffer "bias" parameter that we added - I am still not quite sure what it does
- #j595/journal 
	- [ChatGPT will happily write you a thinly disguised horoscope](https://simonwillison.net/2024/Oct/15/chatgpt-horoscopes/#my-results)
	- [Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent](https://simonwillison.net/2024/Oct/17/video-scraping/)
	- Tested out the TinyLlama1.1B finetuned model
		- Works much better than the SmolLM 360M params - not really sure why but I am not gonna bother trying to figure it out. Will just iterate on the training data for TinyLlama
		- Images
			- ![[2024-10-25-Friday-2024-10-25.png]] ![[2024-10-25-Friday-2024-10-25-1.png]]
	- #j595/meditation - I really like the energy that Corbin & Zi have when they give demos - They just share whatever they have with no pretentiousness of having everything figured out. Is it may be because they are young? I could really use some of that attitude - just show more and show often. I keep trying to wait for a polished demo to show off but that just makes the stakes higher for me. I wish I could cultivate this attitude of "THERE IS NOTHING TOO SMALL TO DEMO". I REALLY need to abandon my judgmental attitude of - "whats the big deal here" and show and celebrate more often. I am feeling increasingly disconnected and disgruntled from the team which is not a good thing - may be I really do need that break.
- #aiml 
	- [Introducing quantized Llama models with increased speed and a reduced memory footprint](https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/)
		- Uses [GitHub - pytorch/executorch: On-device AI across mobile, embedded and edge for PyTorch](https://github.com/pytorch/executorch) as the inference engine (alternative to [[llama-cpp.ipynb]])
		- These models offer a reduced memory footprint, faster on-device inference, accuracy, and portability—all while maintaining quality and safety for developers to deploy on resource-constrained devices. Given the limited runtime memory available on mobile devices, we prioritized short-context applications up to 8K for these new quantized models. Our results show we can achieve superior accuracy by training with quantization as opposed to post-processing.
		- We developed these state-of-the-art models using Quantization-Aware Training with LoRA adaptors (QLoRA) to optimize performance in low-precision environments. We also used SpinQuant, a technique that enables us to determine the best possible combination for compression while retaining the most possible quality.
		- To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with low-rank adaptation (LoRA) adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16.
			- ====*This is the better quantization method because it is essentially using the original training dataset (data aware) in a low bit regime - which is better than blidly chopping off bits . It gives the network the opportunity to learn its way with limited memory*====
		- While the method is less accurate than QAT + LoRA, a key advantage of SpinQuant is its portability and ability to operate without requiring access to training datasets, which are often private. It’s an attractive solution for applications where data availability or computational resources are limited
# TASKS COMPLETED TODAY
%% TCT_TEMPLATED_START 2024-10-25 00:00 %%
%% TCT_TEMPLATED_END 2024-10-25 23:59 %%


# NOTES CREATED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.ctime As Created
WHERE file.ctime >= date(substring(this.file.name,0,10)) - dur(1 week) 
AND file.ctime < date(substring(this.file.name,0,10)) 
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```

# NOTES MODIFIED IN THE LAST WEEK
``` dataview
TABLE file.folder AS Folder, file.mtime AS Modified, file.ctime AS Created
WHERE file.mtime >= date(substring(this.file.name,0,10)) - dur(1 week)
AND file.mtime < date(substring(this.file.name,0,10))
AND file.ctime < date(substring(this.file.name,0,10)) - dur(1 week)
AND file.folder != "Daily"
SORT file.mtime ASCENDING
```
---
