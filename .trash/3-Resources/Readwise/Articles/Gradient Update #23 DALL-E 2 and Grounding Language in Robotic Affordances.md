---
aliases: []
tags:
---
# Gradient Update #23: DALL-E 2 and Grounding Language in Robotic Affordances

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article1.be68295a7e40.png)
### Metadata
Author: [[thegradientpub.substack.com]]
Full Title: Gradient Update #23: DALL-E 2 and Grounding Language in Robotic Affordances
Category: #readwise/articles
URL: https://thegradientpub.substack.com/p/gradient-update-23-dall-e-2-and-grounding
Date Highlighted: [[2022-07-01-Friday]]

## Highlights
- Given an image embedding generated by CLIP, the decoder is a diffusion model that generates images that are likely to have produced this embedding. The prior, which may be an autoregressive or diffusion model, samples CLIP image embeddings given input text. Thus, DALL-E 2 generates an image from a text input by first using the prior to generate a CLIP embedding from the text input, and then using the decoder to generate an image from the CLIP embedding.
- The LLM’s internal distribution is used to rank each skill at each step, and choose the skill that has the highest probability of completing the task. This is the “Say” part of the “SayCan” architecture.Secondly, at each step, the robot must also decide if it is feasible to execute a skill in its current environment.
- something I found to be unique in this study was to not use the language model to generate outputs, but to use the semantic knowledge it learned to find relationships between two text inputs. I am excited to see how researchers continue to use language models in similarly novel applications.
