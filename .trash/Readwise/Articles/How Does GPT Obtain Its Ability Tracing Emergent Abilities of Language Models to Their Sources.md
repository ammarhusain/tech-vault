---
aliases: []
tags:
---
# How Does GPT Obtain Its Ability? Tracing Emergent Abilities of Language Models to Their Sources

![rw-book-cover](https://www.notion.so/images/meta/default.png)
### Metadata
Author: [[How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources]]
Full Title: How Does GPT Obtain Its Ability? Tracing Emergent Abilities of Language Models to Their Sources
Category: #readwise/articles
Document Tags: [ [[aiml]], ]
URL: https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1
Date Highlighted: [[2023-04-22-Saturday]]

## Highlights
- There are three important abilities that the initial GPT-3 exhibit:
  Language generation: to follow a prompt and then generate a completion of the given prompt. Today, this might be the most ubiquitous way of human-LM interaction.
  In-context learning: to follow a few examples of a given task and then generate the solution for a new test case. It is interesting to note that, although being a language model, the original GPT-3 paper barely talks about “language modeling” — the authors devoted their writing efforts to their visions of in-context learning, which is the real focus of GPT-3.
  World knowledge: including factual knowledge and commonsense. ([View Highlight](https://read.readwise.io/read/01gymq35kg3rmrb2fjfjt4naf2))
- The source of the in-context learning ability, as well as its generalization behavior, is still elusive. Intuitively, this ability may come from the fact that data points of the same task are ordered sequentially in the same batch during pretraining. Yet there is little study on why language model pretraining induces in-context learning, and why in-context learning behaves so differently than fine-tuning. ([View Highlight](https://read.readwise.io/read/01gymq6mr47pbepzz67bes765b))
---
aliases: []
tags:
---
# How Does GPT Obtain Its Ability? Tracing Emergent Abilities of Language Models to Their Sources

![rw-book-cover](https://www.notion.so/images/meta/default.png)
### Metadata
Author: [[Notion]]
Full Title: How Does GPT Obtain Its Ability? Tracing Emergent Abilities of Language Models to Their Sources
Category: #readwise/articles
Document Tags: [ #readwise/doc/aiml, ]
URL: https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1
Date Highlighted: [[2023-04-22-Saturday]]

## Highlights
- There are three important abilities that the initial GPT-3 exhibit:
  Language generation: to follow a prompt and then generate a completion of the given prompt. Today, this might be the most ubiquitous way of human-LM interaction.
  In-context learning: to follow a few examples of a given task and then generate the solution for a new test case. It is interesting to note that, although being a language model, the original GPT-3 paper barely talks about “language modeling” — the authors devoted their writing efforts to their visions of in-context learning, which is the real focus of GPT-3.
  World knowledge: including factual knowledge and commonsense. ([View Highlight](https://read.readwise.io/read/01gymq35kg3rmrb2fjfjt4naf2))
- The source of the in-context learning ability, as well as its generalization behavior, is still elusive. Intuitively, this ability may come from the fact that data points of the same task are ordered sequentially in the same batch during pretraining. Yet there is little study on why language model pretraining induces in-context learning, and why in-context learning behaves so differently than fine-tuning. ([View Highlight](https://read.readwise.io/read/01gymq6mr47pbepzz67bes765b))

