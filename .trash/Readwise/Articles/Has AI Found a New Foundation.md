---
aliases: []
tags:
---
# Has AI Found a New Foundation?

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article4.6bc1851654a0.png)
### Metadata
Author: [[thegradient.pub]]
Full Title: Has AI Found a New Foundation?
Category: #readwise/articles
URL: https://thegradient.pub/has-ai-found-a-new-foundation/
Date Highlighted: [[2021-09-27-Monday]]

## Highlights
- You train a big neural network (like the well-known GPT-3) on an enormous amount of data, and then you adapt (“fine-tune”) the model to a bunch of more specific tasks (in the words of the report, "a foundation model ...[thus] serves as [part of] the common basis from which many task-specific models are built via adaptation"). The basic model thus serves as the “foundation” (hence the term) of AIs that carry out more specific tasks. The approach started to gather momentum in 2018, when Google developed the natural language processing model called BERT, and it became even more popular with the introduction last year of OpenAI’s GPT-3.
- A foundation, in its usual sense, is the bedrock on which something complex is built. Software applications, for instance, are built on a foundation of hardware, computer architecture, and an operating system. Programmers can expect ways in which to store and retrieve files, receive inputs, provide outputs, and so forth, with essentially perfect reliability. It is possible to build a word processor or a video game or web browser only because certain prerequisites lie reliably underneath.
- If you ask BERT to fill in the word after “A robin is a ____” it correctly answers “bird”. Unfortunately, if you insert the word not (“A robin is not a ___”) you get exactly the same thing. As AI ethics expert Timnit Gebru and her collaborators put it, these systems are “stochastic parrots” that do a decent job of mimicry but utterly lack the depth of understanding that general artificial intelligence will require.
- We can infer from the parts that (other things being equal) she know now possesses a diamond.
    - Note: typo
- result Is systems
    - Note: lower case Is
- In reality, many of the conspicuous successes of AI have used techniques highly specialized to a particular task in a narrow domain. The Watson system that beat Jeopardy! was a hodge-podge of different specialized techniques, combined in a probabilistic superstructure. AlphaGo and AlphaZero were built on a complex, specialized architecture for searching the game tree of adversarial games. The LIBRATUS poker-playing program was built on an even more complex, even more specialized architecture, designed for partial knowledge, probabilistic, betting games. These accomplishments have not been replicated in foundation models, and there is no reason to think that foundation models would, even in principle, be able (on their own) to replicate them.

