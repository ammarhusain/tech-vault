---
aliases: []
tags:
---
# Large Language Models Can Do Jaw-Dropping Things. But Nobody Knows Exactly Why.

![rw-book-cover](https://wp.technologyreview.com/wp-content/uploads/2024/02/240215_llm.jpg?resize=1200,600)
### Metadata
Author: [[Will Douglas Heaven]]
Full Title: Large Language Models Can Do Jaw-Dropping Things. But Nobody Knows Exactly Why.
Category: #readwise/articles
URL: https://www.technologyreview.com/2024/03/04/1089403/large-language-models-amazing-but-nobody-knows-why/?truid=aa6ca2814d337aa87afac67cde970883&utm_source=the_download&utm_medium=email&utm_campaign=the_download.unpaid.engagement&utm_term=Active%20Qualified&utm_content=03-08-2024&mc_cid=15856024eb&mc_eid=8613610fdf
Date Highlighted: [[2024-03-24-Sunday]]

## Highlights
- According to classical statistics, the bigger a model gets, the more prone it is to overfitting. That’s because with more parameters to play with, it’s easier for a model to hit on wiggly lines that connect every dot. This suggests there’s a sweet spot between under- and overfitting that a model must find if it is to generalize. And yet that’s not what we see with big models. The best-known example of this is a phenomenon known as double descent. ([View Highlight](https://read.readwise.io/read/01hsrpdbnrywhqj3nd8akeh1hw))
- For decades, it was believed that error rate went down and then up as models got bigger: picture a U-shaped curve with the sweet spot for generalization at the lowest point. But in 2018, Belkin and his colleagues found that when certain models got bigger, their error rate went down, then up—and then down again  (a double descent, or W-shaped curve). In other words, large models would somehow overrun that sweet spot and push through the overfitting problem, getting even better as they got bigger. ([View Highlight](https://read.readwise.io/read/01hsrpgwtbr30v57w4a32qb56c))
---
aliases: []
tags:
---
# Large Language Models Can Do Jaw-Dropping Things. But Nobody Knows Exactly Why.

![rw-book-cover](https://wp.technologyreview.com/wp-content/uploads/2024/02/240215_llm.jpg?resize=1200,600)
### Metadata
Author: [[Will Douglas Heaven]]
Full Title: Large Language Models Can Do Jaw-Dropping Things. But Nobody Knows Exactly Why.
Category: #readwise/articles
URL: https://www.technologyreview.com/2024/03/04/1089403/large-language-models-amazing-but-nobody-knows-why/?truid=aa6ca2814d337aa87afac67cde970883&utm_source=the_download&utm_medium=email&utm_campaign=the_download.unpaid.engagement&utm_term=Active%20Qualified&utm_content=03-08-2024&mc_cid=15856024eb&mc_eid=8613610fdf
Date Highlighted: [[2024-03-24-Sunday]]

## Highlights
- According to classical statistics, the bigger a model gets, the more prone it is to overfitting. That’s because with more parameters to play with, it’s easier for a model to hit on wiggly lines that connect every dot. This suggests there’s a sweet spot between under- and overfitting that a model must find if it is to generalize. And yet that’s not what we see with big models. The best-known example of this is a phenomenon known as double descent. ([View Highlight](https://read.readwise.io/read/01hsrpdbnrywhqj3nd8akeh1hw))
- For decades, it was believed that error rate went down and then up as models got bigger: picture a U-shaped curve with the sweet spot for generalization at the lowest point. But in 2018, Belkin and his colleagues found that when certain models got bigger, their error rate went down, then up—and then down again  (a double descent, or W-shaped curve). In other words, large models would somehow overrun that sweet spot and push through the overfitting problem, getting even better as they got bigger. ([View Highlight](https://read.readwise.io/read/01hsrpgwtbr30v57w4a32qb56c))

