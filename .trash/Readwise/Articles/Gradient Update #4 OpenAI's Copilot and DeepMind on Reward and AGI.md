---
aliases: []
tags:
---
# Gradient Update #4: OpenAI's Copilot and DeepMind on Reward and AGI

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article2.74d541386bbf.png)
### Metadata
Author: [[thegradientpub.substack.com]]
Full Title: Gradient Update #4: OpenAI's Copilot and DeepMind on Reward and AGI
Category: #readwise/articles
URL: https://thegradientpub.substack.com/p/update-4-openais-copilot-and-deepmind
Date Highlighted: [[2021-09-30-Thursday]]

## Highlights
- CoPilot is powered by an OpenAI GPT-3 model fine-tuned on billions of lines of public code from GitHub, which OpenAI named Codex. Codex is almost exactly the same as GPT-3, with the main distinction being that it was trained (either from scratch or by fine-tuning from a pre-trained model) on code
- “Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, contain-ing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average line length greater than 100, had maximum line length greater than 1000, or contained a small percentage of alphanumeric characters. After filtering, our final dataset totaled 159 GB.”
- Unlike other approaches to building intelligence such as supervised learning (learning from labelled datasets) or unsupervised learning (learning to mimic or compactly represent unlabeled datasets), the authors of this paper believe that the best approach to creating general intelligence is to focus on reinforcement learning methods that learn via interaction with an environment to maximize a reward signal. The authors argue that the process of learning to maximize a reward signal is sufficient to induce learning language, social intelligence, or perception as a side quest to learn to maximize the overall rewardwithout specific optimization for these subgoals.

