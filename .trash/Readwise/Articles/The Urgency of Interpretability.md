---
aliases: []
tags:
---
# The Urgency of Interpretability

![rw-book-cover](https://cdn.prod.website-files.com/67ecbba31246a69e485fdd4b/680aaa9a4bf95ef74ba834b3_urgency-of-interpretability.png)
### Metadata
Author: [[darioamodei.com]]
Full Title: The Urgency of Interpretability
Category: #readwise/articles
URL: https://www.darioamodei.com/post/the-urgency-of-interpretability?utm_source=alphasignal
Date Highlighted: [[2025-07-16-Wednesday]]

## Highlights
- generative AI systems are *grown* more than they are *built*—their internal mechanisms are “emergent” rather than directly designed.  It’s a bit like growing a plant or a bacterial colony: we set the high-level conditions that direct and shape growth[1](https://www.darioamodei.com/post/the-urgency-of-interpretability/#fn:1)
  1 In the case of a plant, this would be water, sunlight, a trellis pointing them in a certain direction, choosing the species of plant, etc.  These things dictate roughly where the plant grows, but its exact shape and growth pattern are impossible to predict, and hard to explain even after they’ve grown.  In the case of AI systems, we can set the basic architecture (usually some variant of the Transformer), the broad type of data they receive, and the high-level algorithm used to train them, but the model’s actual cognitive mechanisms emerge organically from these ingredients, and our understanding of them is poor.  In fact, there are many examples, in both the natural and artificial worlds, of systems we understand (and sometimes control) at the level of principles but not in detail: economies, snowflakes, cellular automata, human evolution, human brain development, and so on.
  , but the exact structure which emerges is unpredictable and difficult to understand or explain. ([View Highlight](https://read.readwise.io/read/01k08bdbvvg32t1mwqr409m577))
