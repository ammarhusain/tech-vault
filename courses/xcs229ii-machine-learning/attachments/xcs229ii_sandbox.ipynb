{"cells":[{"cell_type":"markdown","metadata":{"id":"bD_64TSfk_2N"},"source":["## Ammar's XCS229ii experiments\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"error","timestamp":1639086396981,"user":{"displayName":"Ammar Husain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64","userId":"10057289260862428677"},"user_tz":480},"id":"qG86J6b8rzVA","outputId":"edc84ac4-6b96-461a-b3a9-1deae98d9598"},"outputs":[{"name":"stderr","output_type":"stream","text":["UsageError: Line magic function `%tensorflow_version` not found.\n"]}],"source":["# Stable Baselines only supports tensorflow 1.x for now\n","#%tensorflow_version 1.x\n","# !pip uninstall -y stable-baselines3[mpi]\n","# !pip install stable-baselines3[mpi]==2.10.0\n","!pip install git+https://github.com/DLR-RM/stable-baselines3.git\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","# function to show an image\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1639086397114,"user":{"displayName":"Ammar Husain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64","userId":"10057289260862428677"},"user_tz":480},"id":"6jpKTy5HU979","outputId":"49aec3a1-1fc2-4241-c562-0a7c35068987"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your runtime has 201.4 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb \u003c 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"elapsed":7919,"status":"ok","timestamp":1639086405033,"user":{"displayName":"Ammar Husain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64","userId":"10057289260862428677"},"user_tz":480},"id":"glN6gy1gk_2N","outputId":"4db42c54-ee52-41af-81ff-b734f882f19d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05f1c6c865764359962b67a92bd15443","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Full dataset size:  train=50000 test=10000\n","Use a subset of the training data to train the Hyp-RL agent : train=20000 val=5000\n","Use a subset of the training data to compare RL agent against HypOpt baseline  : train=40000 val=10000\n"]}],"source":["%matplotlib inline\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 8\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","\n","# separate out some training data to train the RL agent\n","half_data_size = int(len(trainset)/2)\n","\n","rl_agent_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*half_data_size)))\n","rl_agent_testset = torch.utils.data.Subset(trainset, range(int(0.8*half_data_size), half_data_size))\n","\n","hyp_opt_trainset = torch.utils.data.Subset(trainset, range(0,int(0.8*len(trainset))))\n","hyp_opt_testset = torch.utils.data.Subset(trainset, range(int(0.8*len(trainset)), len(trainset)))\n","\n","print(f\"Full dataset size:  train={len(trainset)} test={len(testset)}\")\n","print(f\"Use a subset of the training data to train the Hyp-RL agent : train={len(rl_agent_trainset)} val={len(rl_agent_testset)}\")\n","\n","print(f\"Use a subset of the training data to compare RL agent against HypOpt baseline  : train={len(hyp_opt_trainset)} val={len(hyp_opt_testset)}\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1639086405065,"user":{"displayName":"Ammar Husain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64","userId":"10057289260862428677"},"user_tz":480},"id":"dGn-0O64lHqh","outputId":"35ccee08-50b5-4fc2-de6d-3dead2a8ec3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["## function to train and evaluate the model given the hyperparameter setting\n","\n","## define the neural network\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","def evaluateFullDataset(hp_learning_rate=0.001):\n","  full_train = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","  full_test = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","  net = Net()\n","  loss_criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(net.parameters(), lr=hp_learning_rate, momentum=0.9)\n","  trainAndEvaluateModel(net, loss_criterion, optimizerm, rl_agent_train, rl_agent_test)\n","\n","def trainAndEvaluateModel(net, loss_criterion, optimizer, train, test):\n","  ## Train the model\n","  for epoch in range(2):  # loop over the dataset multiple times\n","\n","      running_loss = 0.0\n","      for i, data in enumerate(train, 0):\n","          # get the inputs; data is a list of [inputs, labels]\n","          inputs, labels = data\n","\n","          # zero the parameter gradients\n","          optimizer.zero_grad()\n","\n","          # forward + backward + optimize\n","          outputs = net(inputs)\n","          loss = loss_criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","          # print statistics\n","          running_loss += loss.item()\n","          if i % 2000 == 1999:    # print every 2000 mini-batches\n","              # print('[%d, %5d] loss: %.3f' %\n","              #       (epoch + 1, i + 1, running_loss / 2000))\n","              running_loss = 0.0\n","  #print('Finished Training')\n","\n","  ## Test the model\n","\n","  # # print images\n","  # dataiter = iter(test)\n","  # images, labels = dataiter.next()\n","  # imshow(torchvision.utils.make_grid(images))\n","  # print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n","  # outputs = net(images)\n","\n","  correct = 0\n","  total = 0\n","  # since we're not training, we don't need to calculate the gradients for our outputs\n","  with torch.no_grad():\n","      for data in test:\n","          images, labels = data\n","          # calculate outputs by running images through the network \n","          outputs = net(images)\n","          # the class with the highest energy is what we choose as prediction\n","          _, predicted = torch.max(outputs.data, 1)\n","          total += labels.size(0)\n","          correct += (predicted == labels).sum().item()\n","\n","  print(f\"Accuracy of the network on the {len(test)} test images: {(100 * correct / total)}%\")\n","  return (100 * correct / total)\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"ALe425tvo_mb"},"source":["## Build the RL environment and agent"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":101,"status":"ok","timestamp":1639086405167,"user":{"displayName":"Ammar Husain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhK_5ZuOt0ZUtSnd8Qf5pN727P_0i1wLPU6ixPb=s64","userId":"10057289260862428677"},"user_tz":480},"id":"7t4pXmwpo-tG"},"outputs":[],"source":["import numpy as np\n","import gym\n","from gym import spaces\n","import copy\n","\n","   \n","class TunableHP:\n","  def __init__(self, train_set, eval_set):\n","    self.hyperparameters = {\"learning_rate\":[0.0001, 0.001, 0.01, 0.1, 1.0],\n","                            \"batch_size\": [2,4, 6,8]}\n","\n","    #self.hyperparameters = {\"learning_rate\":[-5,-4,-3,-2,-1,0,-1,-2,-3,-4,-5]}\n","    self.hyperparameter_keys = list(self.hyperparameters)\n","\n","    self.train_set = train_set\n","    self.eval_set = eval_set\n","\n","  def mapStateToHP(self,state):\n","    hp_dict = {}\n","    for p,i in enumerate(state):\n","      param_key = self.hyperparameter_keys[p]\n","      hp_dict[param_key] = self.hyperparameters[param_key][i]\n","    return hp_dict\n","  \n","  def getGridSize(self):\n","    return [len(self.hyperparameters[k]) for k in self.hyperparameter_keys]\n","\n","  def evaluateRLAgent(self, hp_dict):\n","    print(f\"Running evaluation for : {hp_dict}\")\n","    rl_agent_train = torch.utils.data.DataLoader(self.train_set, batch_size=hp_dict['batch_size'],\n","                                            shuffle=True, num_workers=2)\n","    rl_agent_test = torch.utils.data.DataLoader(self.eval_set, batch_size=hp_dict['batch_size'],\n","                                          shuffle=False, num_workers=2)\n","    net = Net()\n","    loss_criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(net.parameters(), lr=hp_dict['learning_rate'], momentum=0.9)\n","    return trainAndEvaluateModel(net, loss_criterion, optimizer, rl_agent_train, rl_agent_test)\n","\n","class HypRLGridEnv(gym.Env):\n","  \"\"\"\n","  Custom Environment that follows gym interface.\n","  This is a simple env where the agent must learn to go always left. \n","  \"\"\"\n","  # Because of google colab, we cannot implement the GUI ('human' render mode)\n","  metadata = {'render.modes': ['console']}\n","  MAX_ITER = 10\n","\n","  def __init__(self, tunableParams=TunableHP(rl_agent_trainset, rl_agent_testset)):\n","    super(HypRLGridEnv, self).__init__()\n","\n","    self.tunableParams = tunableParams\n","\n","    # Size of the grid\n","    self.grid_size = tunableParams.getGridSize()\n","    \n","    # Define action and observation space\n","    # They must be gym.spaces objects\n","    # Example when using discrete actions, we have two: left and right\n","    n_actions = 3\n","    self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.grid_size),), dtype=np.int32)\n","    # The observation will be the coordinate of the agent\n","    # this can be described both by Discrete and Box space\n","    self.observation_space = spaces.MultiDiscrete(self.grid_size)\n","    self.eval_cache = np.zeros(self.grid_size)\n","\n","  def eval(self, state):\n","    state = tuple(state)\n","    if self.eval_cache[state] == [0.0]:\n","      # train \u0026 test the model for these hyperparameters\n","      self.eval_cache[state] = self.tunableParams.evaluateRLAgent(self.tunableParams.mapStateToHP(state))\n","    return self.eval_cache[state]\n","\n","  def reset(self):\n","    \"\"\"\n","    Important: the observation must be a numpy array\n","    :return: (np.array) \n","    \"\"\"\n","    # reset the number of iterations for this agent\n","    self.iter = 0\n","    # Initialize the agent at the right of the grid\n","    self.agent_state = np.random.randint(self.grid_size)\n","    self.reward = self.eval(self.agent_state)\n","    self.best = {'state':copy.deepcopy(self.agent_state), 'val':self.eval(self.agent_state)}\n","    self.visited = {}\n","    self.visited[tuple(self.agent_state)] = True\n","    return np.array(self.agent_state) \n","\n","  def step(self, action):\n","    self.iter += 1\n","\n","    for i, _ in enumerate(action):\n","      self.agent_state[i] += action[i]\n","      # Account for the boundaries of the grid\n","      self.agent_state[i] = np.clip(self.agent_state[i], 0, self.grid_size[i]-1)\n","\n","    # We are done when we visit the same state twice or have taken more iterations than MAX\n","    done = bool(self.iter \u003e= self.MAX_ITER or tuple(self.agent_state) in self.visited)\n","\n","    self.visited[tuple(self.agent_state)] = True\n","\n","    # reward idea #1\n","    # Reward is minimum of whatever val loss we saw so far\n","    self.reward = max(self.reward, self.eval(self.agent_state))\n","    # Null reward everywhere except when the episode terminates\n","    reward = self.reward if done else 0\n","\n","    # reward idea #2\n","    # set the reward to that observed in the final state\n","    #reward = self.eval(self.agent_state) if done else 0\n","\n","    # reward idea #3\n","    # let the agent accumulate reward as it goes\n","    # self.reward += self.eval(self.agent_state)\n","    # reward = self.reward\n","\n","    if self.eval(self.agent_state) \u003e self.best['val']:\n","      self.best = {'state':copy.deepcopy(self.agent_state), 'val':self.eval(self.agent_state)}\n","\n","    # Optionally we can pass additional info\n","    info = {}\n","    info['best'] = self.best\n","    info['visited'] = self.visited\n","\n","    return np.array(self.agent_state), reward, done, info\n","\n","  def render(self, mode='console'):\n","    if mode != 'console':\n","      raise NotImplementedError()\n","    # agent is represented as a cross, rest as a dot\n","    print(\".\" * self.agent_state, end=\"\")\n","    print(\"x\", end=\"\")\n","    print(\".\" * (self.grid_size - self.agent_state))\n","\n","  def close(self):\n","    pass\n","\n","# check and make sure the environment is sane and working\n","#from stable_baselines.common.env_checker import check_env\n","\n","# If the environment doesn't follow the interface, an error will be thrown\n","# env = HypRLGridEnv()\n","# check_env(env, warn=True)\n","#env.render()"]},{"cell_type":"markdown","metadata":{"id":"AnOq39NCtzbi"},"source":["### RL Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"p_gxuc5NtwcW"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running evaluation for : {'learning_rate': 0.001, 'batch_size': 4}\n","Accuracy of the network on the 1250 test images: 47.4%\n","Running evaluation for : {'learning_rate': 0.001, 'batch_size': 2}\n","Accuracy of the network on the 2500 test images: 43.06%\n","Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 2}\n","Accuracy of the network on the 2500 test images: 35.06%\n","Running evaluation for : {'learning_rate': 0.1, 'batch_size': 2}\n","Accuracy of the network on the 2500 test images: 10.74%\n","Running evaluation for : {'learning_rate': 0.001, 'batch_size': 6}\n","Accuracy of the network on the 834 test images: 42.92%\n","Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 6}\n","Accuracy of the network on the 834 test images: 15.78%\n","Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 8}\n","Accuracy of the network on the 625 test images: 13.06%\n","Running evaluation for : {'learning_rate': 0.001, 'batch_size': 8}\n","Accuracy of the network on the 625 test images: 39.66%\n","Running evaluation for : {'learning_rate': 0.01, 'batch_size': 4}\n","Accuracy of the network on the 1250 test images: 28.88%\n","Running evaluation for : {'learning_rate': 0.01, 'batch_size': 6}\n","Accuracy of the network on the 834 test images: 37.14%\n","Running evaluation for : {'learning_rate': 0.1, 'batch_size': 6}\n","Accuracy of the network on the 834 test images: 10.04%\n","Running evaluation for : {'learning_rate': 1.0, 'batch_size': 2}\n","Accuracy of the network on the 2500 test images: 10.04%\n","Running evaluation for : {'learning_rate': 0.1, 'batch_size': 8}\n","Accuracy of the network on the 625 test images: 10.22%\n","Running evaluation for : {'learning_rate': 0.01, 'batch_size': 2}\n","Accuracy of the network on the 2500 test images: 9.92%\n","Running evaluation for : {'learning_rate': 1.0, 'batch_size': 4}\n","Accuracy of the network on the 1250 test images: 10.06%\n","Running evaluation for : {'learning_rate': 1.0, 'batch_size': 6}\n","Accuracy of the network on the 834 test images: 10.04%\n","Running evaluation for : {'learning_rate': 0.1, 'batch_size': 4}\n","Accuracy of the network on the 1250 test images: 9.98%\n","Running evaluation for : {'learning_rate': 0.01, 'batch_size': 8}\n","Accuracy of the network on the 625 test images: 43.78%\n","Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 4}\n","Accuracy of the network on the 1250 test images: 20.38%\n","Running evaluation for : {'learning_rate': 1.0, 'batch_size': 8}\n","Accuracy of the network on the 625 test images: 10.04%\n"]},{"data":{"text/plain":["\u003cstable_baselines3.a2c.a2c.A2C at 0x7f2f18fe6a10\u003e"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from stable_baselines3 import DQN, PPO, A2C\n","from stable_baselines3.common.env_util import make_vec_env\n","#from stable_baselines3.common.policies import MlpPolicy\n","import pdb\n","# # Instantiate the env\n","env = HypRLGridEnv()\n","# wrap it\n","env = make_vec_env(lambda: env, n_envs=1)\n","\n","# Train the agent\n","##model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)\n","model = A2C('MlpPolicy', env, verbose=0)\n","model.learn(total_timesteps=25000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wb0mvA28dKRz"},"outputs":[{"name":"stdout","output_type":"stream","text":["obs= [[2 2]] reward= [0.] done= [False] info [{'best': {'state': array([2, 2]), 'val': 37.14}, 'visited': {(3, 1): True, (2, 2): True}}]\n","obs= [[1 1]] reward= [0.] done= [False] info [{'best': {'state': array([1, 1]), 'val': 47.4}, 'visited': {(3, 1): True, (2, 2): True, (1, 1): True}}]\n","Goal reached! reward= [47.4] final_state= [2 2] best= {'state': array([1, 1]), 'val': 47.4}\n","info [{'best': {'state': array([1, 1]), 'val': 47.4}, 'visited': {(3, 1): True, (2, 2): True, (1, 1): True}, 'episode': {'r': 47.4, 'l': 3, 't': 1173.046669}, 'terminal_observation': array([2, 2])}]\n","[[35.060 20.380 15.780 13.060]\n"," [43.060 47.400 42.920 39.660]\n"," [9.920 28.880 37.140 43.780]\n"," [10.740 9.980 10.040 10.220]\n"," [10.040 10.060 10.040 10.040]]\n"]}],"source":["# Test the trained agent for sanity checking on the same environment\n","\n","obs = env.reset()\n","n_steps = 20\n","for step in range(n_steps):\n","  action, _ = model.predict(obs, deterministic=True)\n","  # print(\"Step {}\".format(step + 1))\n","  # print(\"Action: \", action)\n","  #pdb.set_trace()\n","  obs, reward, done, info = env.step(action)\n","  if done:\n","    # Note that the VecEnv resets automatically\n","    # when a done signal is encountered\n","    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'], \"best=\", info[0]['best'])\n","    print(f\"info {info}\")\n","    break\n","  print('obs=', obs, 'reward=', reward, 'done=', done, 'info', info)\n","  #env.render(mode='console')\n","\n","np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n","print(f\"{env.envs[0].eval_cache}\")"]},{"cell_type":"markdown","metadata":{"id":"9oEd0rDpAYHI"},"source":["## Time to perform"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IUKMx8hSuUTx"},"outputs":[],"source":["# Instantiate a full environment\n","env_real = HypRLGridEnv(TunableHP(hyp_opt_trainset, hyp_opt_trainset))\n","# wrap it\n","env_real = make_vec_env(lambda: env_real, n_envs=1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8CsUbWSWDyPH"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 8}\n","Accuracy of the network on the 5000 test images: 21.8875%\n","obs= [[0 3]]\n","Step 1\n","Action:  [[-1.000 -1.000]]\n","Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 6}\n","Accuracy of the network on the 6667 test images: 26.0325%\n","obs= [[0 2]] reward= [0.000] done= [False]\n","Step 2\n","Action:  [[-1.000 -1.000]]\n","Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 4}\n","Accuracy of the network on the 10000 test images: 35.5425%\n","obs= [[0 1]] reward= [0.000] done= [False]\n","Step 3\n","Action:  [[-1.000 -0.297]]\n","Running evaluation for : {'learning_rate': 0.0001, 'batch_size': 2}\n","Accuracy of the network on the 20000 test images: 43.1475%\n","obs= [[0 0]] reward= [0.000] done= [False]\n","Step 4\n","Action:  [[-1.000 -1.000]]\n","Goal reached! reward= [43.147] final_state= [0 0]\n","[[43.148 35.542 26.032 21.887]\n"," [0.000 0.000 0.000 0.000]\n"," [0.000 0.000 0.000 0.000]\n"," [0.000 0.000 0.000 0.000]\n"," [0.000 0.000 0.000 0.000]]\n"]}],"source":["# Test the trained agent on a new and full environment of the same dataset\n","obs = env_real.reset()\n","print('obs=', obs)\n","n_steps = 20\n","for step in range(n_steps):\n","  action, _ = model.predict(obs, deterministic=True)\n","  print(\"Step {}\".format(step + 1))\n","  print(\"Action: \", action)\n","  #pdb.set_trace()\n","  obs, reward, done, info = env_real.step(action)\n","  if done:\n","    # Note that the VecEnv resets automatically\n","    # when a done signal is encountered\n","    print(\"Goal reached!\", \"reward=\", reward, \"final_state=\", info[0]['terminal_observation'])\n","    break\n","  print('obs=', obs, 'reward=', reward, 'done=', done)\n","print(env_real.envs[0].eval_cache)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rR8AqJ35cHtA"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[43.148 35.542 26.032 21.887]\n"," [0.000 0.000 0.000 0.000]\n"," [0.000 0.000 0.000 0.000]\n"," [0.000 0.000 0.000 0.000]\n"," [0.000 0.000 0.000 0.000]]\n","obs= [[0 1]]\n"]}],"source":["print(env_real.envs[0].eval_cache)\n","obs = env_real.reset()\n","print('obs=', obs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fEnq60GncQSz"},"outputs":[{"name":"stdout","output_type":"stream","text":["fold 0 ... train 45000 ... test 5000\n","fold 1 ... train 45000 ... test 5000\n","fold 2 ... train 45000 ... test 5000\n","fold 3 ... train 45000 ... test 5000\n","fold 4 ... train 45000 ... test 5000\n","fold 5 ... train 45000 ... test 5000\n","fold 6 ... train 45000 ... test 5000\n","fold 7 ... train 45000 ... test 5000\n","fold 8 ... train 45000 ... test 5000\n","fold 9 ... train 45000 ... test 5000\n"]},{"data":{"text/plain":["10"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import KFold\n","kfold = KFold(n_splits=10, shuffle=True)\n","kfold.split(trainset)\n","for fold, (train_ids, test_ids) in enumerate(kfold.split(trainset)):\n","  print(f\"fold {fold} ... train {len(train_ids)} ... test {len(test_ids)}\")\n","\n","kfold.get_n_splits()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"xcs229ii_sandbox.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05f1c6c865764359962b67a92bd15443":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_115e5fb1ad7e4221923d2099762b2ac2","IPY_MODEL_63e430a5a9d44e3d979e80ea3ae86f13"],"layout":"IPY_MODEL_303983ae6ba74ba4bb7a2f2479d8921c"}},"115e5fb1ad7e4221923d2099762b2ac2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2a4bd1167e74f449cf3479bb78ca262","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6a49cbc954c4594bab295eb98a20ea0","value":1}},"246d04617ff2481dabe7b7e8e0c5be96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"303983ae6ba74ba4bb7a2f2479d8921c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63e430a5a9d44e3d979e80ea3ae86f13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_246d04617ff2481dabe7b7e8e0c5be96","placeholder":"â€‹","style":"IPY_MODEL_9f0b71f5396b47839d8c50284fd74a04","value":" 170500096/? [00:07\u0026lt;00:00, 24275802.40it/s]"}},"9f0b71f5396b47839d8c50284fd74a04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6a49cbc954c4594bab295eb98a20ea0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"f2a4bd1167e74f449cf3479bb78ca262":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}