---
created: 2022-09-05-Monday 21:45
modified: 2023-03-10-Friday 23:15
---

[[courses/xcs224u - natural language understanding/attachments/XCS224U_Mod9_Slides.pdf]]

1. Notebook:Â [Bringing contextual word representations into your models](https://github.com/cgpotts/cs224u/blob/2020-spring/contextualreps.ipynb)

BERT: vocabluary size is like ~30500 which is tiny. But it relies heavily on sub-words. Use their tokenizer when using BERT rather than some other tokenization scheme.
