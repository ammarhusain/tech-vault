---
aliases: []
created: 2022-09-01-Thursday 15:38
modified: 2023-03-10-Friday 23:15
tags: aiml, aiml/nlp, courses/stanford
---


---

[[courses/xcs224n - natural language processing/attachments/XCS224N_Lecture15_Slides.pdf]]

---

![450](courses/xcs224n%20-%20natural%20language%20processing/attachments/image55.png)![450](courses/xcs224n%20-%20natural%20language%20processing/attachments/image9.png)

![450](courses/xcs224n%20-%20natural%20language%20processing/attachments/image72.png)![450](courses/xcs224n%20-%20natural%20language%20processing/attachments/image80.png)

- Temperature in softmax means dividing the scores/logits by a constant T before exponentiating. This affects the output probability distribution. Raising temperature makes the distribution more uniform (spread around the vocab) while lowering temperature makes the distribution more spiky (probability is concentrated on top words).
