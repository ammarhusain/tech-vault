---
aliases: []
created: 2022-09-01-Thursday 13:53
modified: 2023-03-10-Friday 23:15
tags: courses/xcs224n, aiml, aiml/nlp, courses/stanford
---


---

# 1 - Introduction & Word Vectors

[[courses/xcs224n - natural language processing/attachments/XCS224N_Lecture1_Slides.pdf]]
[[courses/xcs224n - natural language processing/attachments/cs224n-lecture_notes_i.pdf]]
Suggested Readings:

1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
2. [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf) (original word2vec paper)
3. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) (negative sampling paper)

Gensim word vectors example: [Gensim word vector visualization](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/materials/Gensim%20word%20vector%20visualization.html)
Commented source code for Word2Vec: [https://github.com/chrisjmccormick/word2vec_commented](https://www.google.com/url?q=https://github.com/chrisjmccormick/word2vec_commented&sa=D&source=editors&ust=1661785131567398&usg=AOvVaw00q6txVkweDQ-L4GO2e2wC)
 NLTK book: [https://www.nltk.org/book/ch02.html](https://www.google.com/url?q=https://www.nltk.org/book/ch02.html%23fig-inaugural2&sa=D&source=editors&ust=1661785131567690&usg=AOvVaw2Aat_LfUMiivW9Ia-W_4dd)

# 2 - Word Vectors 2 and Word Senses

[[courses/xcs224n - natural language processing/attachments/XCS224N_Lecture2_Slides.pdf]]
[[courses/xcs224n - natural language processing/attachments/cs224n-lecture_notes_ii.pdf]]
Suggested Readings:

1. [GloVe: Global Vectors for Word Representation](http://nlp.stanford.edu/pubs/glove.pdf) (original GloVe paper)
2. [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016)
3. [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036)
Additional Readings:
4. [A Latent Variable Model Approach to PMI-based Word Embeddings](http://aclweb.org/anthology/Q16-1028)
5. [Linear Algebraic Structure of Word Senses, with Applications to Polysemy](https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320)
6. [On the Dimensionality of Word Embedding.](https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf)
